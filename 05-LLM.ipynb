{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4462e4",
   "metadata": {},
   "source": [
    "### Motivación\n",
    "Con esta lección finalizamos la parte introductoria del curso. Entender como funciona un LLM nos puede ayudar a comprender porqué se produce esta absurda \"conversación\"  (Perplexity pro mayo 2025):\n",
    "\n",
    "![chat con perplexity](imgs/perplexity1.jpg)\n",
    "\n",
    "En el primer intento responde al saludo pero no con los requisitos en el número de palabras y letras. En el segundo respeta los requerimientos numéricos pero contesta con una frase sin sentido. Por si no se le ha ocurrido, una respuesta muy sencilla y lógica que cumple con lo que se pide en la pregunta es:\n",
    "\n",
    "> ¡Muy bien! ¿Y tú?\n",
    "\n",
    "Otra posibilidad es:\n",
    "\n",
    "> Yo no voy mal.\n",
    "\n",
    "Y también está la que diría mi hijo:\n",
    "\n",
    "> Yo ni tan mal.\n",
    "\n",
    "Pues bien, seguí chateando una rato con respuestas cada vez absurdas de Perplexity hasta que se \"dio por vencido\" y me dijo:\n",
    "\n",
    "\n",
    "![chat con perplexity](imgs/perplexity3.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Los **Modelos de Lenguaje de Gran Escala (LLM)** representan la culminación de décadas de avances en el **Procesamiento del Lenguaje Natural (NLP)**. Estos modelos son capaces de generar texto, responder preguntas, resumir documentos, traducir idiomas, y muchas otras tareas lingüísticas con una calidad notable. Para comprender su arquitectura y funcionamiento, es esencial explorar sus raíces técnicas y conceptuales.\n",
    "\n",
    "El procesamiento del lenguaje natural (NLP) tiene una larga historia que abarca desde métodos simbólicos y basados en reglas hasta técnicas estadísticas y, más recientemente, enfoques de aprendizaje profundo. El NLP surgió como disciplina para permitir a las máquinas **comprender, interpretar y generar lenguaje humano**.\n",
    "\n",
    "Inicialmente, los sistemas NLP eran construidos a mano con gramáticas y reglas lingüísticas específicas. Sin embargo, estos sistemas eran frágiles y difíciles de escalar. Con la llegada de la era estadística, comenzaron a utilizarse grandes corpus de texto para extraer patrones y modelar el lenguaje con probabilidades.\n",
    "\n",
    "Uno de los primeros avances clave fue el uso de **modelos n-grama**, donde la probabilidad de una palabra se modela en función de las anteriores:\n",
    "\n",
    "$\n",
    "P(w_1, w_2, ..., w_n) \\approx \\prod_{i=1}^{n} P(w_i | w_{i-1}, ..., w_{i-k+1})\n",
    "$\n",
    "\n",
    "Estos modelos, aunque efectivos para ciertas tareas, sufrían de problemas de *esparsidad* y una capacidad limitada para capturar dependencias a largo plazo.\n",
    "\n",
    "**El Concepto de Token y Corpus**\n",
    "\n",
    "Para entrenar un modelo de lenguaje, es necesario definir los elementos básicos sobre los que opera. Un **token** es una unidad mínima de texto que el modelo procesa. Dependiendo del enfoque, un token puede ser una palabra, una subpalabra o incluso un carácter. Los LLM modernos usan técnicas como *Byte-Pair Encoding* (BPE) o *WordPiece* para tokenizar texto en subunidades lingüísticas que balancean vocabulario reducido y expresividad.\n",
    "\n",
    "Un **corpus** es simplemente una colección extensa de textos. Para que un modelo adquiera un conocimiento rico del lenguaje, debe exponerse a un corpus masivo y diverso. LLM como GPT o BERT se entrenan sobre cientos de gigabytes o incluso terabytes de texto, provenientes de libros, páginas web, artículos científicos y otras fuentes.\n",
    "\n",
    "**Embeddings: Representaciones Vectoriales del Lenguaje**\n",
    "\n",
    "Una explicación sencilla de lo que es un embedding se puede consultar [aquí](https://cohere.com/llmu).\n",
    "\n",
    "Para que una red neuronal procese texto, las palabras o tokens deben representarse en un formato numérico. Los **embeddings** son vectores densos de números reales que capturan propiedades semánticas y sintácticas de las palabras. En lugar de usar representaciones dispersas como *one-hot vectors*, los embeddings permiten una codificación continua del lenguaje.\n",
    "\n",
    "Una técnica pionera en esta área fue **Word2Vec**, que entrena vectores de palabras de manera que las palabras con contextos similares tengan representaciones vectoriales cercanas. Esto dio lugar encontrar que los embeddings pueden capturar relaciones semánticas complejas. Por ejemplo, la relación entre \"rey\", \"hombre\" y \"reina\", \"mujer\" puede representarse como:\n",
    "\n",
    "$\n",
    "\\text{vec}(\"king\") - \\text{vec}(\"man\") + \\text{vec}(\"woman\") \\approx \\text{vec}(\"queen\")\n",
    "$\n",
    "\n",
    "Los LLM actuales aprenden sus propios embeddings como parte del proceso de entrenamiento, lo que les permite adaptar las representaciones vectoriales a la tarea y arquitectura específicas.\n",
    "\n",
    "**Redes Neuronales Recurrentes y sus Limitaciones**\n",
    "\n",
    "Los primeros intentos de aplicar técnicas NLP en redes neuronales fueron las Redes Neuronales Recurrentes (RNN).\n",
    "Una Red Neuronal Recurrente (RNN) es un tipo de arquitectura de red neuronal artificial diseñada específicamente para procesar datos secuenciales. A diferencia de las redes neuronales feedforward tradicionales, donde la información fluye en una única dirección desde la entrada hasta la salida, las RNN incorporan conexiones recurrentes, lo que les permite mantener un estado interno o memoria de la información procesada previamente en la secuencia. Esta capacidad de \"recordar\" información pasada es fundamental para modelar datos donde el orden y las dependencias temporales son importantes, como el lenguaje natural, series de tiempo, secuencias de ADN o señales de audio y video.\n",
    "\n",
    "Imagina una secuencia de palabras en una frase. Para entender el significado de una palabra, a menudo necesitamos considerar las palabras que la preceden. Una RNN simula esta capacidad manteniendo una unidad de estado oculto (hidden state) que se actualiza a medida que procesa cada elemento de la secuencia. Este estado oculto actúa como una memoria que encapsula información relevante del historial de la secuencia hasta el punto actual. Sin embargo, las RNN tienen dificultades para capturar dependencias a largo plazo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9416a",
   "metadata": {},
   "source": [
    "**El Mecanismo de Atención y el Transformer**\n",
    "\n",
    "El paradigma del procesamiento secuencial experimentó una revolución con la publicación de \"Attention is All You Need\" (Vaswani et al., 2017), artículo seminal que introdujo la arquitectura Transformer. Rompiendo con el procesamiento recursivo tradicional, el Transformer empoderó a cada token para interactuar directamente con todos los demás a través de su innovador mecanismo de atención.\n",
    "\n",
    "Por otra parte, el **Transformer** es como una arquitectura especial de una red neuronal que utiliza este mecanismo de atención de una manera muy inteligente y poderosa. Imagina que antes, las computadoras leían las frases o las secuencias de información palabra por palabra, como si estuvieran leyendo un libro en voz alta, una página a la vez. Esto hacía que fuera difícil recordar cosas que estaban muy lejos en la secuencia.\n",
    "\n",
    "El Transformer es diferente. Es como si pudiera leer toda la página de golpe y, al mismo tiempo, saber qué palabras son más importantes para entender el significado general y las relaciones entre ellas.\n",
    "\n",
    "Piensa en un equipo de personas trabajando juntas para entender una frase larga:\n",
    "\n",
    "Antes (con otras redes neuronales): Una persona leía la frase en voz alta, palabra por palabra, y trataba de recordar todo lo que había escuchado para entender el significado al final. Esto era lento y la persona podía olvidar detalles importantes del principio.\n",
    "\n",
    "Con el Transformer: Imagina que tienes varias personas (las \"cabezas de atención\") que leen la frase al mismo tiempo. Cada persona se enfoca en diferentes partes de la frase y en cómo se relacionan esas partes entre sí. Luego, comparten sus \"atenciones\" para obtener una comprensión completa de la frase.\n",
    "\n",
    "Existen tres tipos principales de modelos basados en transformers, cada uno con aplicaciones distintas. Los modelos de lenguaje autorregresivos, como la serie GPT de OpenAI, predicen el siguiente token en una secuencia basándose únicamente en los tokens precedentes. Estos modelos son excelentes para tareas que implican la generación de texto.\n",
    "\n",
    "Los modelos de lenguaje de auto codificación, como BERT de Google, adoptan un enfoque diferente al predecir tokens basándose en el contexto circundante, lo que los hace bidireccionales. Esta naturaleza bidireccional permite que estos modelos destaquen en tareas como la clasificación de texto, el análisis de sentimientos y el reconocimiento de entidades nombradas. Comprenden eficazmente el contexto completo de una oración. Esto ayuda a mejorar la precisión en la comprensión del significado y la intención detrás del texto.\n",
    "\n",
    "El tercer tipo de modelo transformador combina técnicas autorregresivas y de auto codificación. Un ejemplo de esto es el modelo T5, que puede ajustarse para diversas tareas, aprovechando las fortalezas de ambos enfoques para lograr un rendimiento de vanguardia en una variedad de aplicaciones de NPL.\n",
    "\n",
    "\n",
    "### Estado del arte (mayo 2025)\n",
    "\n",
    "En el momento de escribir estas líneas, parece que un LLM comprende nuestras palabras. Incluso he oído afirmar que algunos los sienten más comprensivos que a sus amigos, manteniendo conversaciones más profundas. Si bien me parece inquietante equiparar una máquina con un ser humano, es crucial recordar que los LLM no procesan ni generan información como nosotros. \n",
    "\n",
    "Para ilustrar la verdadera capacidad de comprensión de un LLM, he realizado un sencillo ejercicio que cualquier persona resolvería sin esfuerzo. Consideremos la siguiente frase incompleta:\n",
    " \n",
    "> Mañana voy a llevar el coche al _________.\n",
    "\n",
    "La respuesta intuitiva, y la predicha unánimemente por varios modelos de lenguaje avanzados a mayo de 2025, es:\n",
    "\n",
    "> Mañana voy a llevar el coche al taller.\n",
    "\n",
    "Sin embargo, la fragilidad de su comprensión contextual se evidencia al insertar la misma frase en un escenario ligeramente más complejo:\n",
    "\n",
    "> Estoy harto de que mi coche no pare de tener averías. Ya he visto el modelo de coche que me quiero comprar. Mañana voy a llevar el coche al _________.\n",
    "\n",
    "Aquí, el contexto se enriquece con un coche problemático y la intención de adquirir uno nuevo. La frase a completar se refiere claramente al primer vehículo. Un modelo que solo predice la siguiente palabra basándose en las adyacentes, como las arquitecturas pre-Transformer, inevitablemente fallará al capturar esta implicación contextual. El mecanismo de atención, diseñado para identificar relaciones significativas entre palabras distantes, debería, en teoría, permitir al modelo \"entender\" la conexión entre el hartazgo con las averías y la decisión de comprar un coche nuevo, infiriendo así el destino más probable del vehículo averiado.\n",
    "\n",
    "Mi respuesta intuitiva en este contexto sería:\n",
    "\n",
    "> Mañana voy a llevar el coche al desguace.\n",
    "\n",
    "Sorprendentemente, al someter esta misma prueba a los modelos de lenguaje actuales (mayo de 2025), ninguno ofreció esta predicción. Las respuestas variaron entre el predecible \"taller\" y el, en este contexto, igualmente desacertado \"concesionario\". Esta discrepancia entre la comprensión humana, que procesa el contexto de forma holística, y la respuesta de los LLM, incluso con la sofisticación del mecanismo de atención, subraya una limitación fundamental. A pesar de su habilidad para identificar palabras clave, la inferencia contextual profunda, que implica comprender intenciones y relaciones implícitas a través de múltiples oraciones, sigue siendo un desafío significativo.\n",
    "\n",
    "Este ejemplo sirve como un recordatorio crucial de la naturaleza y los límites de los Large Language Models. Comprender sus mecanismos internos y sus inherentes limitaciones es esencial para una aplicación responsable y realista de estas tecnologías, evitando la ilusión de una comprensión semántica completa donde aún existen notables carencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d459868",
   "metadata": {},
   "source": [
    "### ¿Escribir como Cervantes?\n",
    "\n",
    "En esta sección vamos a entrenar un Red Neuronal Recurrente (RNN) para que escriba como Cervantes. Para ello, vamos a utilizar como corpus su obra más famosa y entrenar una RNN para que genere texto similar al estilo del autor. No se deben esperar resultados espectaculares, ya que tanto la red elegida como el limitado corpus no son los más adecuados para este tipo de tareas. Sin embargo, es un buen ejercicio para entender cómo funcionan las RNN y cómo pueden ser utilizadas para generar texto. Para ejecutar esta parte del notebook es conveniente hacerlo en Google Colab y con una GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f249a8c",
   "metadata": {},
   "source": [
    "1. Preprocesamiento del corpus\n",
    "Descarga y normalización del texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8371e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1. Setup and Data Preprocessing ----------\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Download and preprocess text\n",
    "url = \"https://www.gutenberg.org/cache/epub/2000/pg2000.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "# Extract main content\n",
    "start = text.find(\"Primera parte del ingenioso hidalgo\")\n",
    "end = text.find(\"Fin de la obra\")\n",
    "clean_text = text[start:end].lower().replace('\\n', ' ').replace('\\r', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127255\n"
     ]
    }
   ],
   "source": [
    "# longitud del texto\n",
    "print(len(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limitamos el texto a 100000 caracteres para acelerar el entrenamiento\n",
    "clean_text = clean_text[:100000]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb728b",
   "metadata": {},
   "source": [
    "Hacemos una tokenización sencilla a nivl de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1793031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2. Character Tokenization ----------\n",
    "# Create vocabulary\n",
    "chars = sorted(list(set(clean_text)))\n",
    "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "# Create training sequences\n",
    "seq_length = 100\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(clean_text) - seq_length, 1):\n",
    "    X_seq = clean_text[i:i+seq_length]\n",
    "    y_seq = clean_text[i+1:i+seq_length+1]  # Shifted sequence\n",
    "    X.append([char_to_idx[c] for c in X_seq])\n",
    "    y.append([char_to_idx[c] for c in y_seq])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b6eab",
   "metadata": {},
   "source": [
    "Definimos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c274257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CervantesLSTM(\n",
      "  (embedding): Embedding(48, 64)\n",
      "  (lstm): LSTM(64, 128, num_layers=3, dropout=0.3)\n",
      "  (fc): Linear(in_features=128, out_features=48, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3. Model Architecture (LSTM) ----------\n",
    "class CervantesLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, \n",
    "                          num_layers=3, dropout=0.3,\n",
    "                          batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "# Initialize model\n",
    "model = CervantesLSTM(len(chars)).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a459fe1",
   "metadata": {},
   "source": [
    "Realizamos el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad90fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m targets = batch_y.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous().view(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m logits, hidden = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Detach hidden states\u001b[39;00m\n\u001b[32m     32\u001b[39m hidden = \u001b[38;5;28mtuple\u001b[39m(h.detach() \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hidden)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ampliacion_entornos/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ampliacion_entornos/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mCervantesLSTM.forward\u001b[39m\u001b[34m(self, x, hidden)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[32m     12\u001b[39m     emb = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     out, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.fc(out)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, hidden\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ampliacion_entornos/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ampliacion_entornos/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ampliacion_entornos/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---------- 4. Training Configuration ----------\n",
    "# Convert data to tensors and move to device\n",
    "X_tensor = torch.LongTensor(X).to(device)\n",
    "y_tensor = torch.LongTensor(y).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Training parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# ---------- 5. Training Loop with GPU Support ----------\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = None\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Prepare inputs and targets\n",
    "        hidden = None\n",
    "        inputs = batch_X.transpose(0, 1).contiguous()  # (seq_len, batch)\n",
    "        targets = batch_y.transpose(0, 1).contiguous().view(-1)  # Flatten\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Detach hidden states\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits.view(-1, len(chars)), targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass with gradient clipping\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deda2e8",
   "metadata": {},
   "source": [
    "Generamos texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3abdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6. Text Generation Function ----------\n",
    "def generate_text(model, seed_str, length=1000, temp=0.8):\n",
    "    model.eval()\n",
    "    generated = [char_to_idx[c] for c in seed_str]\n",
    "    input_seq = torch.LongTensor(generated).unsqueeze(1).to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(input_seq, hidden)\n",
    "            \n",
    "            # Focus on last character\n",
    "            logits = logits[-1] / temp\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            generated.append(next_idx)\n",
    "            \n",
    "            # Update input\n",
    "            input_seq = torch.LongTensor([next_idx]).unsqueeze(1).to(device)\n",
    "    \n",
    "    return ''.join([idx_to_char[i] for i in generated])\n",
    "\n",
    "# Example generation\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generate_text(model, \"en un lugar de la mancha\", temp=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac98452",
   "metadata": {},
   "source": [
    "El texto generado es un sin sentido. Pero si lo analizamos un poco podemos ver que hay muchas palabras que existen en castellano. Este resultado ya en sí es interesante. Debemos tener en cuenta que hemos tokenizado en base a caracteres. Es decir, que la red predice un carácter a partir de los anteriores. Así que no es un ejercicio trivial genera una palabra completa. Además se deben generar los espacios entre palabras y el final de oración."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
